---
title: "Statistical Learning mod.B Project (Miami House Price Prediction)"
author: 'Provided by: Sina Ainesazi Dovom - Bahar Khanshaghaghi'
date: "July 2022"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

**Contents**

1.  Introduction

    1.1. Objectives of the study

2.  Preparation of the Dataset

    2.1. Collection of the Data

    2.2. Preprocessing Data

    2.3. Data Exploration and Data Analysis

3.  Modelling and Data analysis

    3.1. Numerical Variables

    3.2. Categorical Variables

4.  Final Model

    4.1. Final Model selection

    4.2. Cross-Validation of the Final model

# 1. Introduction

## 1.1 objectives of the study 
We used a data source found on Kaggle.com for this study. This dataset is titled "Miami Housing Dataset". The dataset contains information on 13,932 single-family homes sold in Miami of USA. Our goal is to obtain a regression model using 17 variables inside the dataset to predict the prices of the houses in Miami. We will proceed by a simple linear regression model with only the most important variable and then by checking on all the other explanatory variables, we will try to select the most useful ones and try to check which degree of polynomial is better for each of them using the response variable which is the price of the houses.

# 2. Preparation of the Dataset

Importing the required Libraries

```{r}
library(ggplot2)
library(leaps)
library(boot)
library(Metrics)
library(knitr)
```

## 2.1. Collection of the Dataset

The dataset has 18 features giving information contains information on 13,932 single-family homes sold in Miami, USA.

```{r}
# Importing the Dataset #
house = 'miami-housing.csv'
Miami_house <- read.csv(house)
head(Miami_house)
attach(Miami_house)
```

First look at the Dataset

```{r}
str(Miami_house)
```

Variables in the Dataset and details related to them are as follows:

-   PARCELNO: unique identifier for each property. About 1% appear multiple times.

-   SALE_PRC: sale price (\$)

-   LND_SQFOOT: land area (square feet)

-   TOT_LVG_AREA: floor area (square feet)

-   SPEC_FEAT_VAL: value of special features (e.g., swimming pools) (\$)

-   RAIL_DIST: distance to the nearest rail line (an indicator of noise) (feet)

-   OCEAN_DIST: distance to the ocean (feet)

-   WATER_DIST: distance to the nearest body of water (feet)

-   CNTR_DIST: distance to the Miami central business district (feet)

-   SUBCNTR_DI: distance to the nearest subcenter (feet)

-   HWY_DIST: distance to the nearest highway (an indicator of noise) (feet)

-   age: age of the structure

-   avno60plus: dummy variable for airplane noise exceeding an acceptable level

-   structure_quality: quality of the structure

-   month_sold: sale month in 2016 (1 = jan)

-   LATITUDE

-   LONGITUDE

## 2.2.Preprocessing Data

Checking the dimension of the dataset.

```{r}
dim(Miami_house)
```

Dropping the ID columns since it has no specific information

```{r}
Miami_house$PARCELNO = NULL
```

Checking to see if there are null values in the dataset.

```{r}
sum(is.na(Miami_house))
```

So, there are no null values in the dataset.

Taking a look at the Target Variable

```{r}
summary(Miami_house$SALE_PRC)
```

The distribution of the Target variable in this study is not normal it will be displayed using histogram in the data exploration part. When our original continuous data do not follow the bell curve, we can log transform this data to make it as "normal" as possible so that the statistical analysis results from this data become more valid. In other words, the log transformation reduces or removes the skewness of our original data.

```{r}
Miami_house$log10_SALE_PRC = log10(SALE_PRC)
```

The Categorical variables which have been converted to factors in this study are: 1. avno60plus which is a dummy variable for airplane noise exceeding an acceptable level.

2.  month_sold which shows the sale month of the structure in 2016 (1 = jan).

3.  structure_quality which has a value between 1 to 5 and shows the quality of the structure.

```{r}
Miami_house$avno60plus = as.factor(Miami_house$avno60plus)
Miami_house$month_sold = as.factor(Miami_house$month_sold)
Miami_house$structure_quality = as.factor(Miami_house$structure_quality)
```

Two new columns added to the Dataset, indicating if the house has a specific feature like swimming pool and if the house has a body of water(there is a river or a lake inside the property or the property is very close to a river or a lake.)

```{r}
Miami_house$has_SPECFEAT = as.factor(Miami_house$SPEC_FEAT_VAL!=0)
Miami_house$has_BODYOFWATER = as.factor(Miami_house$WATER_DIST==0)
```

Taking a look at the dimension and the new columns added to the dataset, after taking the Preprocessing actions.

```{r}
dim(Miami_house)
str(Miami_house)
```

## 2.3.Data Exploration and Data Analysis

Checking on the distribution of the values of the variables using plots.

```{r, fig.height=5, fig.width=10}
# Checking the distribution of SALE PRICE variable #
ggplot(Miami_house, aes(x= SALE_PRC)) + geom_histogram(color = "green", bins=30) +
  labs(x ="price ($)", title = "Price of the House")
```

This histogram shows that the majority of the instances inside this dataset have prices less than 1 million dollars and the response variable in this study does not have a normal distribution.

```{r, fig.height=5, fig.width=10}
# Logaritmic distribution of SALE PRICE variable #
ggplot(Miami_house, aes(x= log10_SALE_PRC)) + geom_histogram(color = "green", bins=30) + 
  labs(x ="logaritmic price ($)", title = "Price of the House (Log)")
```

This histogram shows the distribution of the new response variable which is the logarithmic type of the previous target variable. And perfectly shows the normally distribution of the values.

```{r, fig.height=5, fig.width=10}
# Checking istribution of LAND SIZE variable #
ggplot(Miami_house, aes(x = LND_SQFOOT)) + geom_histogram(color = "green", bins=30) + 
  labs(x ="land area (square feet)", title = "Size of Land")
```

This histogram is for the variable related to the size of the land in which the house is located and it's obvious that the majority of the instances have the land area less the 20 thousand square feet.

```{r, fig.height=5, fig.width=10}
# Checking distribution of FLOOR SIZE variable #
ggplot(Miami_house, aes(x = TOT_LVG_AREA)) + geom_histogram(color = "green",bins=30) + 
  labs(x ="floor area (square feet)", title = "Size of Floor")
```

This histogram is for the variable related to the size of the floor of the house and it's obvious that the majority of the instances have size less the 4 thousand square feet.

```{r, fig.height=5, fig.width=10}
# Checking distribution of SPECIFIC FEATURE variable #
ggplot(Miami_house, aes(x = SPEC_FEAT_VAL)) + geom_histogram(color = "green", bins=30) + 
  labs(x ="value of special features ($)", title = "Special Feature")
```

The variable related to the specific feature of the house takes two kind of values which are 0 if there is no specific feature in the house or a number which indicated the price of the specific feature in the house if there is exist any specific feature. which this histogram shows that around 2000 of the houses have no specific features and around 7000 have a specefic feature with low value .

```{r, fig.height=5, fig.width=10}
# Checking distribution of DISTANCE from RAILWAY variable #
ggplot(Miami_house, aes(x = RAIL_DIST)) + geom_histogram(color = "green", bins=30) + 
  labs(x ="distance to the nearest rail line (feet)", title = "Rail line Dist")
```

This variable related to the distance to the nearest railway is an indicator of the noise and this histogram shows that around 1200 of the houses have distance less than 5000 feet and about 400 of the houses are very close to a railway and they're supposed to feel and hear the noise of the train.

```{r, fig.height=5, fig.width=10}
# Checking distribution of DISTANCE from OCEAN variable #
ggplot(Miami_house, aes(x = OCEAN_DIST)) + geom_histogram(color = "green", bins=30) +
  labs(x ="distance to the ocean (feet)", title = "Ocean Dist")
```

This histogram shows the distance of houses from ocean in feet, and it shows that the majority of the instances have distance from ocean about 15000 to 40000 feet.

```{r, fig.height=5, fig.width=10}
# Checking distribution of DISTANCE from WATER variable #
ggplot(Miami_house, aes(x = WATER_DIST)) + geom_histogram(color = "green", bins=30) + 
  labs(x ="distance to the nearest body of water (feet)", title = "Water Dist")

```

This histogram shows the distance of a house to the nearest body of water like river and lake and it's obvious that about 1400 of the houses are very close to a body of water and the majority of the houses have a distance of about 5000 feet.

```{r, fig.height=5, fig.width=10}
# Checking distribution of DISTANCE from BUSINESS CENTER variable #
ggplot(Miami_house, aes(x = CNTR_DIST)) + geom_histogram(color = "green", bins=30) + 
  labs(x ="distance to the Miami central business district (feet)", 
       title = "Central Business Dist")
```

This variable shows the distance of houses to the Miami central business district in feet and this histogram shows that most of the houses have distances about 40000 to 100000 feet from the Miami central business districtt.

```{r, fig.height=5, fig.width=10}
# Checking distribution of DISTANCE from SUBCENTER variable #
ggplot(Miami_house, aes(x = SUBCNTR_DI)) + geom_histogram(color = "green", bins=30) + 
  labs(x ="distance to the nearest subcenter (feet)", 
       title = "SUBCENTER Dist")
```

From this histogram it's obvious that the majority of the houses have distances around 50000 feet from the nearest subcenter in miami.

```{r, fig.height=5, fig.width=10}
# Checking distribution of DISTANCE from HIGHWAY variable #
ggplot(Miami_house, aes(x = HWY_DIST)) + geom_histogram(color = "green", bins=30) + 
  labs(x ="distance to the nearest highway (feet)", 
       title = "HIGHWAY Dist")
```

The variable HWY_DIST is related to the distance to the nearest highway which is an indicator of noise. This histogram shows that about 800 of the houses are very close to a highway and they may hear noise and most of the houses have distances about 5000 feet to a highway.

```{r, fig.height=5, fig.width=10}
# Borplot of the age variable #
ggplot(Miami_house, aes(x = age)) + geom_bar(color="yellow") + 
  labs(x ="Age of house")
```

This variable age is about the number of years that the house has been built and from this barplot it can be seen that about 1000 of the houses are very new and the small amount of the houses have age over 75 years.

```{r, fig.height=5, fig.width=10}
# Borplot of airplane noise variable #
ggplot(Miami_house, aes(x = avno60plus)) + geom_bar(color="yellow") + 
  labs(x ="Aiplane noise")
```

The variable avno60plus is an indicator of airplane noise. This barplot shows that in a small number of houses the noise of the airplane exceeds an acceptable level.

```{r, fig.height=5, fig.width=10}
# Boxplot of the price with respect to the noise of airplane #
ggplot(Miami_house, aes(x=avno60plus, y=log10_SALE_PRC, 
fill=factor(avno60plus))) + geom_boxplot(alpha=0.3) + theme(legend.position="none") + 
labs(x = 'airplane noise', y = 'Log(Price)')
```

This boxplot shows the connection between the price of the houses and the variable related to the noise of the airplane. It's obvious that although there are very small number of houses which the airplane noise exceeds an acceptable level but it does not have any negative effect on the price of the house.

```{r, fig.height=5, fig.width=10}
# Borplot of SOLD MONTH variable #
ggplot(Miami_house, aes(x = month_sold)) + geom_bar(color="yellow") + 
  labs(x ="Month",  title = "Sold Month")
```

The variable month_sold shows the sale month of the house in 2016 (1 = jan). and this barplot shows that the most of the houses have been sold in spring and summer.

```{r, fig.height=5, fig.width=10}
# Boxplot of the price with respect to the sold month #
ggplot(Miami_house, aes(x=month_sold, y=log10_SALE_PRC, 
fill=factor(month_sold))) + 
geom_boxplot(alpha=0.3) + theme(legend.position="none") + 
  labs(x = 'Month', y = 'Log(Price)')
```

This barplot is related to the prices of the houses in different months and it's obvious that the prices are the same in the different months and only the number of the houses sold differ month to month not the price.

```{r, fig.height=5, fig.width=10}
# Borplot of STRUCTURE QUALITY variable #
ggplot(Miami_house, aes(x = structure_quality)) + 
  geom_bar(color="yellow") + labs(x ="Quality",  
title = "Quality of the Structure")
```

The variable structure_quality shows the quality of the structure. and it takes a value between 1 to 5. This barplot shows that most of the houses have quality 4.

```{r, fig.height=5, fig.width=10}
# Boxplot of the price with respect to the quality of the structure#
ggplot(Miami_house, aes(x=structure_quality, 
y=log10_SALE_PRC, fill=factor(structure_quality))) + 
geom_boxplot(alpha=0.3) + 
theme(legend.position="none") + 
  labs(x = 'Quality', y = 'Log(Price)')
```

This boxplot shows that the price of the house increases by the quality value of the house. and this variable has strong effect on the price of the house.

```{r, fig.height=5, fig.width=10}
# Borplot of has specific feature variable #
ggplot(Miami_house, aes(x = has_SPECFEAT)) + geom_bar(color="yellow") + 
labs(x ="If there is specific feature")
```

This variable has been made from the variable related to the specific feature of the house. which shows that most of the houses have a speific feature.

```{r, fig.height=5, fig.width=10}
# Boxplot of the price and if there is specific feature #
ggplot(Miami_house, aes(x=has_SPECFEAT, y=log10_SALE_PRC, fill=factor(has_SPECFEAT))) + 
geom_boxplot(alpha=0.3) + 
  theme(legend.position="none") +
  labs(x = 'There is specific feature', y = 'Log(Price)')
```

This boxplot shows the connection between the price and the feature of the houses. and it shows that the houses with specific feature have higher prices but, it's obvious that there is not much difference between the price of the houses with specific feature and without a specifit feature.

```{r, fig.height=5, fig.width=10}
# Borplot of has BODYOFWATER variable #
ggplot(Miami_house, aes(x = has_BODYOFWATER)) + geom_bar(color="yellow") + 
  labs(x ="If there is waterbody")
```

This variable shows that if the house has a river or a lake and the most of the instances have no body of water.

```{r, fig.height=5, fig.width=10}
# Boxplot of the price with respect to the fact that if there is WATERBODY #
ggplot(Miami_house, aes(x=has_BODYOFWATER, 
y=log10_SALE_PRC,
fill=factor(has_BODYOFWATER))) +
  geom_boxplot(alpha=0.3) + theme(legend.position="none") + 
  labs(x = 'There is WATERBODY', y = 'Log(Price)')
```

There is a clear difference between the houses with a body of water and the houses without body of water.

# 3. Modelling and Data analysis

This part of the study consitsts of three steps. Which in the first step only the Numerical features have been checked then in the second step only the variables with categorical type have been taken into account and in the last step the chosen variables of each type have been combined with each other.

## 3.1. Numerical Variables

First, to begin this step thee correlation of the target variable, logaritmic price and other numerical variables have been checked.

```{r}
numerical_variables <- subset(Miami_house, select = 
c(log10_SALE_PRC, LND_SQFOOT, TOT_LVG_AREA, SPEC_FEAT_VAL, RAIL_DIST, 
OCEAN_DIST, WATER_DIST, CNTR_DIST, 
SUBCNTR_DI, HWY_DIST, LATITUDE, LONGITUDE))
round(cor(numerical_variables[]), 3)
```

As it can be seen, there is a high correlation between the variable TOT_LVG_AREA, which is the indicator of the floor area of the house and the logarithm of price. So, it should be checked to see which degree of linear regression is the best.

```{r}
# Linear model #
model.num1 <- lm(data = Miami_house, log10_SALE_PRC ~ TOT_LVG_AREA)
summary(model.num1)
```

```{r}
BIC(model.num1)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = TOT_LVG_AREA, y = log10_SALE_PRC)) +
  geom_point() + stat_smooth(method = "lm", formula = y ~ x) +
  labs(x = 'floor area (square feet)', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic #
par(mfrow=c(2,2))
plot(model.num1)
```

```{r}
par(mfrow=c(1,1))
```

As it can be seen the simple linear regression can somehow interpolate the data. In the plot of the residuals shows that residuals are somehow randomly distributed. And the scale-location plot, which is a type of plot that displays the fitted values of the regression model along the x-axis and the square root of the standardized residuals along the y-axis, shows that although the red line is not flat, but it's somehow horizontal across the plot and the assumption of homoscedasticity is almost satisfied for this regression model. Although that the simple linear model is satisfying but for the sake of an improvement the second degree polynomial model has been tested.

```{r}
# Polynomial model degree = 2 #
model.num2 <- lm(data=Miami_house, log10_SALE_PRC ~ TOT_LVG_AREA + I(TOT_LVG_AREA**2))
summary(model.num2)
```

```{r}
BIC(model.num2)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = TOT_LVG_AREA, y = log10_SALE_PRC)) +
  geom_point() + stat_smooth(method = "lm", formula = y ~ x + I(x**2)) + 
  labs(x = 'floor area (square feet) + floor area^2', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic #
par(mfrow=c(2,2))
plot(model.num2)
```

```{r}
par(mfrow=c(1,1))
```

The regression line now fits a little bit better the data and the residuals plot shows a little improvement and also the scale-location plot is more flat. So, this is the simple model for numerical variables. Next step, is Best Subset Selection step for other numerical variables by using the regsubsets() function which performs best subset selection by identifying the best model.

```{r}
# Using Regsubset() function #
regfit.num <- regsubsets(log10_SALE_PRC ~ LND_SQFOOT + 
TOT_LVG_AREA + SPEC_FEAT_VAL + RAIL_DIST + OCEAN_DIST + 
WATER_DIST + CNTR_DIST + SUBCNTR_DI +
HWY_DIST + LATITUDE + LONGITUDE, data=Miami_house)
reg.summary <- summary(regfit.num)
reg.summary
```

Here the best 8 models can be seen. An asterisk ("\*") indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only TOT_LVG_AREA and SUBCNTR_DI.

```{r}
reg.summary$rsq
```

We see that the R2 statistic increases from 50% whenonly one variable is included in the model to almost 69% when all variables are included. As expected, the R2 statistic increases monotonically as more variables are included.

Plotting RSS, adjusted R2, Cp, and BIC for all of the models at once will help us decide which model to select.

```{r}
par(mfrow=c(2,2))

# residual sum of squares
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS", type="l")

# adjusted-RË†2 with its largest value
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq", type="l")
which.max(reg.summary$adjr2)
points(8, reg.summary$adjr2[8], col="blue",cex=2,pch=20)

# Mallow's Cp with its smallest value
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type='l')
which.min(reg.summary$cp)
points(8, reg.summary$cp[8], col="blue", cex=2, pch=20)

# BIC with its smallest value
plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type='l')
which.min(reg.summary$bic)
points(8, reg.summary$bic[8], col="blue", cex=2, pch=20)
```

```{r}
par(mfrow=c(1,1))
```

The RSS plot shows that the RSS decreases as the number of variables increase and the Adjusted R2 plot shows that it increases as the number of variables increase, The Cp plot admits that it decreases as the number of the variables increase this fact is also true for the BIC plot. The plot() command indicates the selected variables for the best model, ranked according to a chosen statistic. The top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic.

```{r, fig.height=4.5, fig.width=10}
plot(regfit.num,scale="r2")
plot(regfit.num,scale="adjr2")
plot(regfit.num,scale="Cp")
plot(regfit.num,scale="bic")
```

```{r}
# Best model "BIC"
best.bic <- lm(log10_SALE_PRC~TOT_LVG_AREA+SPEC_FEAT_VAL+RAIL_DIST+WATER_DIST+SUBCNTR_DI+HWY_DIST, data=Miami_house)
summary(best.bic)

```

In this study, BIC has been used to evaluate the models in order to obtain a model with lower number of variables. The plots showed the as the number of variables exceed number 6 there is not a big difference between the model with 6 number of variables and the one with 8 variables. So the model in this study has the following variables.

1.  TOT_LVG_AREA

2.  SUBCNTR_DI

3.  SPEC_FEAT_VAL

4.  HWY_DIST

5.  WATER_DIST

6.  RAIL_DIST

The next step, is to check the best degree for each selected variable in order to have a better fit of the regression line in the plot of each variable against the response variable.

```{r}
# log(price) vs Special Feature variable #
model.SPEC_FEAT_VAL <- lm(data=Miami_house, log10_SALE_PRC ~ SPEC_FEAT_VAL)
summary(model.SPEC_FEAT_VAL)
```

```{r}
BIC(model.SPEC_FEAT_VAL)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = SPEC_FEAT_VAL, y = log10_SALE_PRC)) + geom_point()+ stat_smooth(method = "lm", formula = y ~ x) + 
  labs(x = 'Value of special features ($)', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.SPEC_FEAT_VAL)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# log(price) vs SPEC_FEAT_VAL + SPEC_FEAT_VAL2
model.SPEC_FEAT_VAL2 <- lm(data=Miami_house, log10_SALE_PRC ~ SPEC_FEAT_VAL + I(SPEC_FEAT_VAL**2))
summary(model.SPEC_FEAT_VAL2)
BIC(model.SPEC_FEAT_VAL2)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = SPEC_FEAT_VAL, y = log10_SALE_PRC)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x,2)) +
  labs(x = 'Value of special features ($) + SPEC_FEAT_VAL^2', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.SPEC_FEAT_VAL2)
```

These plots show that adding degree improves the interpolation of the data. But to have more proof about this fact the anova function has been used.

```{r}
par(mfrow=c(1,1))
```

```{r}
anova(model.SPEC_FEAT_VAL, model.SPEC_FEAT_VAL2)
```

It can be seen that with having such a small P-value, we can conclude that there is an evidence to believe that model including SPEC_FEAT_VAL\^2 provides a statistically significantly better fit than the model without.

```{r}
# log(price) vs Distance to the nearest RAIL LINE variable #
model.RAIL_DIST <- lm(data=Miami_house, log10_SALE_PRC ~ RAIL_DIST)
summary(model.RAIL_DIST)
```

```{r}
BIC(model.RAIL_DIST)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting the model #
ggplot(Miami_house, aes(x = RAIL_DIST, y = log10_SALE_PRC)) + geom_point() + stat_smooth(method = "lm", formula = y ~ x) + 
labs(x = 'Distance to the nearest RAIL LINE (feet)', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.RAIL_DIST)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
model.RAIL_DIST2 <- lm(data=Miami_house, log10_SALE_PRC ~ RAIL_DIST + I(RAIL_DIST**2))
summary(model.RAIL_DIST2)
```

```{r}
BIC(model.RAIL_DIST2)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = RAIL_DIST, y = log10_SALE_PRC)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x,2)) +
  labs(x = 'Distance to the nearest RAIL LINE (feet) + RAIL_DIST^2', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.RAIL_DIST2)
```

```{r}
par(mfrow=c(1,1))
```

For variable RAIL_DIST a polynomial of grade 2 does not improve the fit so much. Moreover, comparing the residuals plots, the first one seems to be better.

```{r}
# log(price) vs Distance to the nearest BODY OF WATER variable #
model.WATER_DIST <- lm(data=Miami_house, log10_SALE_PRC ~ WATER_DIST)
summary(model.WATER_DIST)
```

```{r}
BIC(model.WATER_DIST)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting mode #
ggplot(Miami_house, aes(x = WATER_DIST, y = log10_SALE_PRC)) + geom_point() + stat_smooth(method = "lm", formula = y ~ x) + labs(x = 'Distance to the nearest BODY OF WATER (feet)', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.WATER_DIST)
```

```{r}
par(mfrow=c(1,1))
```

It seems that there is a relationship more complex than a simple linear model. So, the first 3 polynomial degree have been tested and anova function has been used.

```{r}
model.WATER_DISTmulti <- lm(data=Miami_house, log10_SALE_PRC ~ WATER_DIST + I(WATER_DIST**2) + I(WATER_DIST**3))
anova(model.WATER_DISTmulti)
```

ANOVA function shows that plotting model with polynomial degree equal to 3 is needed.

```{r}
# Degree 2 for Water Distance #
model.WATER_DIST2 <- lm(data=Miami_house, log10_SALE_PRC ~ poly(WATER_DIST, 2))
summary(model.WATER_DIST2)
```

```{r}
BIC(model.WATER_DIST2)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = WATER_DIST, y = log10_SALE_PRC)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2)) +
  labs(x = 'Distance to the nearest BODY OF WATER (feet) + WATER_DIST^2', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.WATER_DIST2)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# Degree 3 for Water Distance #
model.WATER_DIST3 <- lm(data=Miami_house, log10_SALE_PRC ~ poly(WATER_DIST, 3))
summary(model.WATER_DIST3)
```

```{r}
BIC(model.WATER_DIST3)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = WATER_DIST, y = log10_SALE_PRC)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 3)) +
  labs(x = 'Distance to the nearest BODY OF WATER (feet)', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.WATER_DIST3)
```

```{r}
par(mfrow=c(1,1))
```

From the first two plots we can see that a first degree polynomial seems to not fit very well the data but from the residual plot it can be captured that trying to plot a polynomial with degree 3 the data is perfectly fitted by the regression line and the residuals plot shows that there is an equal distribution around the axis. Also, for this variable the second degree polynomial checked but the residual plot shows that the residuals are not randomly distributed and the scale-location plot shows that the horizontal line is not flat.

```{r}
# log(price) vs Distance to the nearest SUBCENTER variable #
model.SUBCNTR_DI <- lm(data=Miami_house, log10_SALE_PRC ~ SUBCNTR_DI)
summary(model.SUBCNTR_DI)
```

```{r}
BIC(model.SUBCNTR_DI)
```

```{r, fig.height=4.5, fig.width=10}
ggplot(Miami_house, aes(x = SUBCNTR_DI, y = log10_SALE_PRC)) + 
  geom_point() + stat_smooth(method = "lm", formula = y ~ x) + 
  labs(x = 'Distance to the nearest Subcenter (feet)', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.SUBCNTR_DI)
```

```{r}
par(mfrow=c(1,1))
```

Regarding this variable also it seems that there is a relationship more complex than a simple linear model. So, the first 3 polynomial degree have been tested and anova function has been used.

```{r}
model.SUBCNTR_DImulti <- lm(data=Miami_house, log10_SALE_PRC ~ SUBCNTR_DI + I(SUBCNTR_DI**2) + I(SUBCNTR_DI**3))
anova(model.SUBCNTR_DImulti)
```

From what ANOVA function showed it was observed that there is a need to plot a model with polynomial degree equal to 2 and 3 and then check more parameters in both of them.

```{r}
# Degree 2 for Subcenter Distance #
model.SUBCNTR_DI2 <- lm(data=Miami_house, log10_SALE_PRC ~ poly(SUBCNTR_DI, 2))
summary(model.SUBCNTR_DI2)
```

```{r}
BIC(model.SUBCNTR_DI2)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = SUBCNTR_DI, y = log10_SALE_PRC)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2)) +
  labs(x = 'Distance to the nearest subcenter (feet) + SUBCNTR_DI^2', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.SUBCNTR_DI2)
```

```{r}
par(mfrow=c(1,1))
```

Second degree polynomial regression fits the data better and the plots show the residuals are equally distributed around zero. The red line in scale-Location plot is flat. We also compare it to the third degreee polynomial regression to select the degree of regression.

```{r}
# Degree 3 for Subcenter Distance #
model.SUBCNTR_DI3 <- lm(data=Miami_house, log10_SALE_PRC ~ poly(SUBCNTR_DI, 3))
summary(model.SUBCNTR_DI3)
```

```{r}
BIC(model.SUBCNTR_DI3)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = SUBCNTR_DI, y = log10_SALE_PRC)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 3)) +
  labs(x = 'Distance to the nearest subcenter (feet)', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.SUBCNTR_DI3)
```

```{r}
par(mfrow=c(1,1))
```

Adding polynomial degree 3 has better fit on the data but looking at residuals plot shows that there is not a good distribution of the residuals and also the red line in scale-location plot is not flat. So, the second degree polynomial has been tested to check all of these factors in that also.

There is a significant difference between the first degree polynomial and the second degree polynomial model. The second degree fits better the data and residuals plot shows the better distribution of the residuals and scale-location plot also shows that the red line is more flat in comparison to the degree polynomial equal to 3.

```{r}
# log(price) vs Distance to the nearest HIGHWAY variable #
model.HWY_DIST <- lm(data=Miami_house, log10_SALE_PRC ~ HWY_DIST)
summary(model.HWY_DIST)
```

```{r}
BIC(model.HWY_DIST)
```

```{r, fig.height=4.5, fig.width=10}
# Plottinf model #
ggplot(Miami_house, aes(x = HWY_DIST, y = log10_SALE_PRC)) + 
geom_point() + stat_smooth(method = "lm", formula = y ~ x) + 
labs(x = 'Distance to the nearest Highway (feet)', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.HWY_DIST)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# Degree 2 for HIGHWAY Distance #
model.HWY_DIST2 <- lm(data=Miami_house, log10_SALE_PRC ~ poly(HWY_DIST, 2))
summary(model.HWY_DIST2)
```

```{r}
BIC(model.HWY_DIST2)
```

```{r, fig.height=4.5, fig.width=10}
# Plotting model #
ggplot(Miami_house, aes(x = HWY_DIST, y = log10_SALE_PRC)) +
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2)) +
  labs(x = 'Distance to the nearest HIGHWAY (feet) + HWY_DIST^2', y = 'Log(Price) ($)')
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.HWY_DIST2)
```

```{r}
par(mfrow=c(1,1))
```

The second degree fits better the data and residuals plot shows the better distribution of the residuals and also scale-location plot shows that the horizontal red line is more flat.

After having done these analysis, there are 5 models which are as follows:

1.  A simple model with only the first degree component of the variable TOT_LVG_AREA.

2.  A simple model with a first and a second degree component of the variable TOT_LVG_AREA.

3.  A model with all the 6 variables with first degree:

    1.  TOT_LVG_AREA

    2.  SUBCNTR_DI

    3.  SPEC_FEAT_VAL

    4.  HWY_DIST

    5.  WATER_DIST

    6.  RAIL_DIST

4.  A model with all the 6 variables:

    1.  TOT_LVG_AREA, with second degree polynomial

    2.  SUBCNTR_DI

    3.  SPEC_FEAT_VAL

    4.  HWY_DIST

    5.  WATER_DIST

    6.  RAIL_DIST

5.  A model with all the 6 variables:

    1.  TOT_LVG_AREA, with second degree polynomial

    2.  SUBCNTR_DI, with second degree polynomial

    3.  SPEC_FEAT_VAL, with second degree polynomial

    4.  HWY_DIST, with second degree polynomial

    5.  WATER_DIST, with third degree polynomial

    6.  RAIL_DIST

In this step all of the numerical models have been compared to each other.

```{r}
# The first numerical model with one variable, (TOT_LVG_AREA) #
summary(model.num1)
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.num1)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# The second numerical model with 2 variables, (TOT_LVG_AREA) and (TOT_LVG_AREA**2)#
summary(model.num2)
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.num2)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# The third numerical model with 6 variables, all the 6 variables with degree 1 #
model.poly1 <- lm(log10_SALE_PRC ~ TOT_LVG_AREA + SPEC_FEAT_VAL +
RAIL_DIST + WATER_DIST + SUBCNTR_DI + HWY_DIST, data=Miami_house)
summary(model.poly1)
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.poly1)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# The 4th numerical model with all the variables with degree 1 except TOT_LVG_AREA with degree 2 #
model.poly2 <- lm(log10_SALE_PRC ~ poly(TOT_LVG_AREA,2) + 
SPEC_FEAT_VAL + RAIL_DIST + WATER_DIST + SUBCNTR_DI +
HWY_DIST, data=Miami_house)
summary(model.poly2)
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.poly2)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# The fifth numerical model with 6 variables, all 6 variables with their best degree #
model.poly3 <- lm(log10_SALE_PRC ~ poly(TOT_LVG_AREA,2) +
poly(SPEC_FEAT_VAL,2) + RAIL_DIST + poly(WATER_DIST, 3) +
poly(SUBCNTR_DI, 2) + poly(HWY_DIST, 2), data=Miami_house)
summary(model.poly3)
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.poly3)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# Comparing BIC #

BIC(model.num1)
BIC(model.num2)
BIC(model.poly1)
BIC(model.poly2)
BIC(model.poly3)
```

This process shows that the fifth model is the best one among other models with the BIC of -17508.44. But all of these models will be checked with each other again when the categorical variables merged with the numerical ones.

# 3.2. Categorical Variables

In this part all the categorical variables in the dataset have been processed. The idea is to study the full model at first and then by removing the variables which are not significant and at the end a model will be selected that has the least number of variables with R2 that is not so different from the R2 which the full model has.

```{r}
# CATEGORICAL VARIABLE SELECTION #
model.cat1 <- lm(log10_SALE_PRC ~ avno60plus + month_sold + 
structure_quality + has_SPECFEAT +has_BODYOFWATER + 
age, data=Miami_house)
summary(model.cat1)
```

```{r}
BIC(model.cat1)
```

```{r}
# without month_sold #
model.cat2 <- lm(log10_SALE_PRC ~ avno60plus + 
structure_quality + has_SPECFEAT +has_BODYOFWATER + 
age, data=Miami_house)
summary(model.cat2)
```

```{r}
BIC(model.cat2)
```

```{r}
# without month_sold and avno60plus #
model.cat3 <- lm(log10_SALE_PRC ~ structure_quality + has_SPECFEAT + has_BODYOFWATER + age, data=Miami_house)
summary(model.cat3)
```

```{r}
BIC(model.cat3)
```

```{r}
# without month_sold and avno60plus and structure #
model.cat4 <- lm(log10_SALE_PRC ~ has_SPECFEAT +
has_BODYOFWATER + age, data=Miami_house)
summary(model.cat4)
```

```{r}
BIC(model.cat4)
```

Seems that the variable structure_quality is a very important variable for the model. So removing it has adverse effect.

```{r}
# without month_sold and has feature and avno60plus #
model.cat5 <- lm(log10_SALE_PRC ~ structure_quality  +
has_BODYOFWATER + age, data=Miami_house)
summary(model.cat5)
```

```{r}
BIC(model.cat5)
```

```{r}
# without month_sold and has water and avno60plus and has feature #
model.cat6 <- lm(log10_SALE_PRC ~ structure_quality + age, data=Miami_house)
summary(model.cat6)
```

```{r}
BIC(model.cat6)
```

```{r}
# without month_sold and has water and avno60plus and has feature and age #
model.cat7 <- lm(log10_SALE_PRC ~ structure_quality, data=Miami_house)
summary(model.cat7)
```

```{r}
BIC(model.cat7)
```

```{r}
# Checking on the R2 of these models #
summary(model.cat1)$adj.r.squared
summary(model.cat2)$adj.r.squared
summary(model.cat3)$adj.r.squared
summary(model.cat4)$adj.r.squared
summary(model.cat5)$adj.r.squared
summary(model.cat6)$adj.r.squared
summary(model.cat7)$adj.r.squared
```

The analysis showed that the variables structure_quality and age are important variables for the model and the model with only containing these two variables has the R2 value of 0.2841 which in comparison to the full model which was 0.3040. There is not a specific difference between these two considering that the full model contained 6 variables but the model.cat6 only contains 2 variables. So the selected model as the best of categorical models in this study is model.cat6.

# Final Model

## 4.1. Final Model selection

In this part the numerical models which contain numerical variables (TOT_LVG_AREA, SPEC_FEAT_VAL, RAIL_DIST, WATER_DIST, SUBCNTR_DI and HWY_DIST) and the categorical model which contain categorical variables (structure_quality and age) will be combined with each other. There are 5 models which are as follows:

1.  Model Poly1 + Cat6

2.  Model Poly2 + Cat6

3.  Model Poly3 + Cat6

```{r}
# Poly1 + CAT6 #
model.Poly1Cat6 <- lm(log10_SALE_PRC ~ TOT_LVG_AREA + 
SPEC_FEAT_VAL + RAIL_DIST + WATER_DIST + SUBCNTR_DI +
HWY_DIST + structure_quality + age, data=Miami_house)
summary(model.Poly1Cat6)
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.Poly1Cat6)
```

```{r}
BIC(model.Poly1Cat6)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# Poly2 + CAT6 #
model.Poly2Cat6 <- lm(log10_SALE_PRC ~ poly(TOT_LVG_AREA,2) + 
SPEC_FEAT_VAL + RAIL_DIST + WATER_DIST + SUBCNTR_DI + 
HWY_DIST + structure_quality + age, data=Miami_house)
summary(model.Poly2Cat6)
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.Poly2Cat6)
```

```{r}
BIC(model.Poly2Cat6)
```

```{r}
par(mfrow=c(1,1))
```

```{r}
# Poly3 + CAT6 #
model.Poly3Cat6 <- lm(log10_SALE_PRC ~ poly(TOT_LVG_AREA,2) + 
poly(SPEC_FEAT_VAL, 2) + RAIL_DIST + poly(WATER_DIST, 3) + 
poly(SUBCNTR_DI, 2) + poly(HWY_DIST, 2) + structure_quality + age, data=Miami_house)
summary(model.Poly3Cat6)
```

```{r}
# Diagnostic
par(mfrow=c(2,2))
plot(model.Poly3Cat6)
```

```{r}
BIC(model.Poly3Cat6)
```

Considering all the three final models which are the combination of numerical models and the categorical model. the first one has the adjusted R2 as 0.7682 and BIC as -19724.21. but the second model has adjusted R2 as 0.7814 and BIC as -20537.78, for the third model also the adjusted R2 is 0.8101 and BIC is -22456.93. which in comparison to the first two models has better results and also considering the residual plot for the third model it's obvious that the residuals are normally distributed around zero axis and also the scale-location plot shows a good distribution and the horizontal line is flat, also the QQ plot is good too, there is some distance on both ends but is acceptable.

## 4.2. Cross-Validation of the Final model

In this part the mean squared error and the 10-fold cross-validation error has been calculated in order to check the performance of the final best model in this study.

Where Mean Squared Error is:$$
MSE = {\frac{1}{n}}\Sigma(y_i-\overline{y_i})^2
$$

and Cross-validation is:

$$
CV_k = \Sigma {\frac{n_k}{n}}MSE_k
$$

```{r}
finalbest.fit <- lm(log10_SALE_PRC ~ poly(TOT_LVG_AREA,2) + 
poly(SPEC_FEAT_VAL, 2) + RAIL_DIST + poly(WATER_DIST, 3) + 
poly(SUBCNTR_DI, 2) + poly(HWY_DIST, 2) + structure_quality +
age, data=Miami_house)
```

```{r}
model.glm <- glm(log10_SALE_PRC ~ poly(TOT_LVG_AREA,2) + 
poly(SPEC_FEAT_VAL, 2) + RAIL_DIST + poly(WATER_DIST, 3) + 
poly(SUBCNTR_DI, 2) + poly(HWY_DIST, 2) + structure_quality +
age, data=Miami_house)
summary(model.glm)
```

```{r}
# Mean Squared Error for final best model #
mse(Miami_house$log10_SALE_PRC, fitted.values(finalbest.fit))
```

```{r}
cv.err <- cv.glm(data = Miami_house, model.glm, K=10)
cv.err$delta[1]
```
The mean squared error for the actual values of the response variable and the fitted values using the best final model is 0.01153 and the 10-fold cross validation error using cv.glm function is 0.01156, which confirms that the best final model that has been obtained in this study is qualified. The final best model has 8 explanatory variables which have been chosen through the analysis in part 3 among 17 variables inside the dataset. This model has the adjusted R2 of about 81% which is an indicator of a high accuracy.